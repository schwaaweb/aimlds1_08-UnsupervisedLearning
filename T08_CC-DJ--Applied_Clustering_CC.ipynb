{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "view-in-github",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_08-UnsupervisedLearning/blob/master/Applied_Clustering_CC.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "TQ0tjkDTQmR_",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Applied Clustering Coding Challenge\n",
    "\n",
    "# Clustering text documents using k-means\n",
    "\n",
    "Clustering is commonly used to create classifications for written text using Natural Language Processing ([NLP](https://learn.lambdaschool.com/ml/sprint/recelerz9zemekmia)) techniques. This approach is highlighted by a combination of supervised and unsupervised techniques - given a huge (and growing) corpus of literature, it may be very hard for observers to classify each document. A clustering approach allows classes to be generated automatically, grouping each document into like clusters. Then, researchers may fit the clustering to specific documents that they wish to get better insigned into. Any document that shares a cluster with the tagged documents of interest then becomes a document of interest.\n",
    "\n",
    "In this CC, we will do feature extraction and clustering on a collection of newgroup documents.\n",
    "\n",
    "Text documents are not directly classifiable by ML algorithms, the must first be \"Vectorized\", that is, turned into numerical vectors by some process. Then each document becomes (as you are now familiar with) a single point in a high dimensional space. Then k-Means or any other ML algorithm can be used for processing. Vectorizing the documents is also called \"feature extraction\": using either of the two below algorithms a text file is converted into a set of features, aka a numerical row in a database, aka a single sample of `X`, aka a point in a high dimensional space.\n",
    "\n",
    "Two feature extraction methods can be used in this example:\n",
    "\n",
    "  - [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) ([Term-Frequency Inverse-Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)) uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.\n",
    "\n",
    "  - [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space.\n",
    "\n",
    "    HashingVectorizer does not provide IDF weighting as this is a stateless\n",
    "    model (the fit method does nothing). When IDF weighting is needed it can\n",
    "    be added by pipelining its output to a TfidfTransformer instance.\n",
    "    \n",
    "Try both of these feature extractors and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "colab": null,
    "colab_type": "code",
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "id": "yDYQSP-gQgBn",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load all categories from the training set\n",
    "categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True) # We randomize the data so the order doesn't bias the data analysis\n",
    "                             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846 documents\n",
      "20 categories\n",
      "\n",
      "labels:  [10  3 17 ...  3  1  7]\n",
      "labels (18846,)\n",
      "Unique Labels:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "true_k:  20\n",
      "CPU times: user 7.12 ms, sys: 4.27 ms, total: 11.4 ms\n",
      "Wall time: 17.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n",
    "\n",
    "labels = dataset.target\n",
    "\n",
    "print('labels: ',labels)\n",
    "print('labels', labels.shape)\n",
    "\n",
    "# How many actual classes are there?\n",
    "true_k = np.unique(labels).shape[0]\n",
    "print('Unique Labels: ', np.unique(labels))\n",
    "print('true_k: ',true_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exemplar:\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      " \n",
      "*******************\n",
      "\n",
      "From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\n",
      "Subject: Which high-performance VLB video card?\n",
      "Summary: Seek recommendations for VLB video card\n",
      "Nntp-Posting-Host: midway.ecn.uoknor.edu\n",
      "Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\n",
      "Keywords: orchid, stealth, vlb\n",
      "Lines: 21\n",
      "\n",
      "  My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n",
      "-- \n",
      "    |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   \n",
      "  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  --+-- \n",
      "    |   of heaven, because everything he does is right and all his ways  |   \n",
      "    |   are just.\" - Nebuchadnezzar, king of Babylon, 562 B.C.           |   \n",
      " \n",
      "*******************\n",
      "\n",
      "From: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Subject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\n",
      "Lines: 95\n",
      "Nntp-Posting-Host: viktoria.dsv.su.se\n",
      "Reply-To: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Organization: Dept. of Computer and Systems Sciences, Stockholm University\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\n",
      "\n",
      "\n",
      "|>Greater Armenia would stretch from Karabakh, to the Black Sea, to the\n",
      "|>Mediterranean, so if you use the term \"Greater Armenia\" use it with care.\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>It has always been up to the Azeris to end their announced winning of Karabakh \n",
      "|>by removing the Armenians! When the president of Azerbaijan, Elchibey, came to \n",
      "|>power last year, he announced he would be be \"swimming in Lake Sevan [in \n",
      "|>Armeniaxn] by July\".\n",
      "\t\t*****\n",
      "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
      "\tOr have you changed your calendar???\n",
      "\n",
      "\n",
      "|>Well, he was wrong! If Elchibey is going to shell the \n",
      "|>Armenians of Karabakh from Aghdam, his people will pay the price! If Elchibey \n",
      "\t\t\t\t\t\t    ****************\n",
      "|>is going to shell Karabakh from Fizuli his people will pay the price! If \n",
      "\t\t\t\t\t\t    ******************\n",
      "|>Elchibey thinks he can get away with bombing Armenia from the hills of \n",
      "|>Kelbajar, his people will pay the price. \n",
      "\t\t\t    ***************\n",
      "\n",
      "\n",
      "\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\n",
      "\t\n",
      "\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\n",
      "\t\t\t\t\t\t    **************\n",
      "\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\n",
      "\t\n",
      "\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\n",
      "\tYOU FACIST!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\tOhhh i forgot, this is how Armenians fight, nobody has forgot\n",
      "\tyou killings, rapings and torture against the Kurds and Turks once\n",
      "\tupon a time!\n",
      "      \n",
      "       \n",
      "\n",
      "|>And anyway, this \"60 \n",
      "|>Kurd refugee\" story, as have other stories, are simple fabrications sourced in \n",
      "|>Baku, modified in Ankara. Other examples of this are Armenia has no border \n",
      "|>with Iran, and the ridiculous story of the \"intercepting\" of Armenian military \n",
      "|>conversations as appeared in the New York Times supposedly translated by \n",
      "|>somebody unknown, from Armenian into Azeri Turkish, submitted by an unnamed \n",
      "|>\"special correspondent\" to the NY Times from Baku. Real accurate!\n",
      "\n",
      "Ohhhh so swedish RedCross workers do lie they too? What ever you say\n",
      "\"regional killer\", if you don't like the person then shoot him that's your policy.....l\n",
      "\n",
      "\n",
      "|>[HE]\tSearch Turkish planes? You don't know what you are talking about.<-------\n",
      "|>[HE]\tsince it's content is announced to be weapons? \t\t\t\ti\t \n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "|>Well, big mouth Ozal said military weapons are being provided to Azerbaijan\ti\n",
      "|>from Turkey, yet Demirel and others say no. No wonder you are so confused!\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\tConfused?????\t\t\t\t\t\t\t\ti\n",
      "\tYou facist when you delete text don't change it, i wrote:\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "        Search Turkish planes? You don't know what you are talking about.\ti\n",
      "        Turkey's government has announced that it's giving weapons  <-----------i\n",
      "        to Azerbadjan since Armenia started to attack Azerbadjan\t\t\n",
      "        it self, not the Karabag province. So why search a plane for weapons\t\n",
      "        since it's content is announced to be weapons?   \n",
      "\n",
      "\tIf there is one that's confused then that's you! We have the right (and we do)\n",
      "\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\n",
      " \n",
      "\n",
      "|>You are correct, all Turkish planes should be simply shot down! Nice, slow\n",
      "|>moving air transports!\n",
      "\n",
      "\tShoot down with what? Armenian bread and butter? Or the arms and personel \n",
      "\tof the Russian army?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hilmi Eren\n",
      "Stockholm University\n",
      " \n",
      "*******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Dataset exemplar:')\n",
    "\n",
    "for n in range(3):\n",
    "    print(dataset.data[n],'\\n*******************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "1oJYuDLrUfPU",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Specify the method of feature extraction and other parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "colab": null,
    "colab_type": "code",
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "id": "sEZ0Hy_3Umuz",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "use_hashing = True\n",
    "use_idf = False\n",
    "n_features = 10000\n",
    "n_components = 0\n",
    "minibatchKMeans = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "2RhQWW9xXOdc",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Define the feature extractor class and apply it to the  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "colab": null,
    "colab_type": "code",
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "id": "tiFv3PexXTn-",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.766258s\n",
      "n_samples: 18846, n_features: 10000\n",
      "\n",
      "CPU times: user 6.75 s, sys: 147 ms, total: 6.9 s\n",
      "Wall time: 8.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "\n",
    "if use_hashing:\n",
    "    if use_idf:\n",
    "        # Perform an IDF normalization on the output of HashingVectorizer\n",
    "        hasher = HashingVectorizer(\n",
    "                                   n_features=n_features,\n",
    "                                   stop_words='english', \n",
    "                                   alternate_sign=False,\n",
    "                                   norm=None, \n",
    "                                   binary=False\n",
    "                                  )\n",
    "        vectorizer = make_pipeline(hasher, \n",
    "                                    TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, \n",
    "                                       norm='l2',\n",
    "                                       binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                                 max_features=n_features,\n",
    "                                 min_df=2, \n",
    "                                 stop_words='english',\n",
    "                                 use_idf=use_idf)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "-5rHJtfpXaA0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Potentially, also do dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "colab": null,
    "colab_type": "code",
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "id": "SX8F3RGbXd9E",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "if n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "l1Sj87MrX1-j",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Then do the actual clustering and output clustering metrics\n",
    "\n",
    "Look at using [MiniBatchKmeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) insead of regular KMeans - MiniBatch means to use a smaller subset of the data over a few passes and average their results. MiniBatch has a shorter runtime than full KMeans and can actually complete on very-large datasets.\n",
    "\n",
    "Compare the clustering performance metrics between using KMeans and MiniBatchKMeans:\n",
    "* Homogeneity\n",
    "* Completeness\n",
    "* V-measure\n",
    "* Adjusted Rand-Index\n",
    "* Silhouette\n",
    "\n",
    "After you've finished examining the differing performance between KMeans and MiniBatchKMeans, try using a `k` other than `true_k`.\n",
    "\n",
    "Suggestion: Write a loop that computes and outputs the Clustering Metrics for $k \\in \\{1...30\\}$. Graph one or more of the metrics with $k$ as the `x` axis. We should see them maximize around $k=20$, the true number of clusters. Do we?\n",
    "\n",
    "Finally, after clustering, draw some exemplar data samples from each cluster and compare them in your own mind. Is the clustering process working for separating out the content of different emails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "colab": null,
    "colab_type": "code",
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "id": "DHOeGxRdX39c",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your implementation of Clustering Execution and Performance Metrics\n",
    "\n",
    "if minibatchKMeans:\n",
    "   km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1, init_size=1000, batch_size=1000)\n",
    "else:\n",
    "   km = KMeans(n_\n",
    "   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": null,
   "name": "Applied Clustering CC.ipynb",
   "provenance": null,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "T08_CC-DJ--Applied_Clustering_CC.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
