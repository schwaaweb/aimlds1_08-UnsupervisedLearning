{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Applied Clustering CC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_08-UnsupervisedLearning/blob/master/Applied_Clustering_CC.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "TQ0tjkDTQmR_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Applied Clustering Coding Challenge\n",
        "\n",
        "# Clustering text documents using k-means\n",
        "\n",
        "Clustering is commonly used to create classifications for written text using Natural Language Processing ([NLP](https://learn.lambdaschool.com/ml/sprint/recelerz9zemekmia)) techniques. This approach is highlighted by a combination of supervised and unsupervised techniques - given a huge (and growing) corpus of literature, it may be very hard for observers to classify each document. A clustering approach allows classes to be generated automatically, grouping each document into like clusters. Then, researchers may fit the clustering to specific documents that they wish to get better insigned into. Any document that shares a cluster with the tagged documents of interest then becomes a document of interest.\n",
        "\n",
        "In this CC, we will do feature extraction and clustering on a collection of newgroup documents.\n",
        "\n",
        "Text documents are not directly classifiable by ML algorithms, the must first be \"Vectorized\", that is, turned into numerical vectors by some process. Then each document becomes (as you are now familiar with) a single point in a high dimensional space. Then k-Means or any other ML algorithm can be used for processing. Vectorizing the documents is also called \"feature extraction\": using either of the two below algorithms a text file is converted into a set of features, aka a numerical row in a database, aka a single sample of `X`, aka a point in a high dimensional space.\n",
        "\n",
        "Two feature extraction methods can be used in this example:\n",
        "\n",
        "  - [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) ([Term-Frequency Inverse-Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)) uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.\n",
        "\n",
        "  - [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space.\n",
        "\n",
        "    HashingVectorizer does not provide IDF weighting as this is a stateless\n",
        "    model (the fit method does nothing). When IDF weighting is needed it can\n",
        "    be added by pipelining its output to a TfidfTransformer instance.\n",
        "    \n",
        "Try both of these feature extractors and compare them:"
      ]
    },
    {
      "metadata": {
        "id": "yDYQSP-gQgBn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
        "#         Lars Buitinck\n",
        "# License: BSD 3 clause\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# #############################################################################\n",
        "# Load all categories from the training set\n",
        "categories = None\n",
        "\n",
        "print(\"Loading 20 newsgroups dataset for categories:\")\n",
        "print(categories)\n",
        "\n",
        "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
        "                             shuffle=True)\n",
        "\n",
        "print(\"%d documents\" % len(dataset.data))\n",
        "print(\"%d categories\" % len(dataset.target_names))\n",
        "print()\n",
        "\n",
        "labels = dataset.target\n",
        "\n",
        "print('Dataset exemplar:')\n",
        "print(dataset.data[0])\n",
        "\n",
        "# How many actual classes are there?\n",
        "true_k = np.unique(labels).shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1oJYuDLrUfPU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Specify the method of feature extraction and other parameters\n"
      ]
    },
    {
      "metadata": {
        "id": "sEZ0Hy_3Umuz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_hashing = True\n",
        "use_idf = True\n",
        "n_features = 10000\n",
        "n_components = 0\n",
        "minibatchKMeans = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2RhQWW9xXOdc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the feature extractor class and apply it to the  dataset"
      ]
    },
    {
      "metadata": {
        "id": "tiFv3PexXTn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
        "t0 = time()\n",
        "\n",
        "if use_hashing:\n",
        "    if use_idf:\n",
        "        # Perform an IDF normalization on the output of HashingVectorizer\n",
        "        hasher = HashingVectorizer(n_features=n_features,\n",
        "                                   stop_words='english', alternate_sign=False,\n",
        "                                   norm=None, binary=False)\n",
        "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
        "    else:\n",
        "        vectorizer = HashingVectorizer(n_features=n_features,\n",
        "                                       stop_words='english',\n",
        "                                       alternate_sign=False, norm='l2',\n",
        "                                       binary=False)\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=n_features,\n",
        "                                 min_df=2, stop_words='english',\n",
        "                                 use_idf=use_idf)\n",
        "X = vectorizer.fit_transform(dataset.data)\n",
        "\n",
        "print(\"done in %fs\" % (time() - t0))\n",
        "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-5rHJtfpXaA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Potentially, also do dimensionality reduction"
      ]
    },
    {
      "metadata": {
        "id": "SX8F3RGbXd9E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if n_components:\n",
        "    print(\"Performing dimensionality reduction using LSA\")\n",
        "    t0 = time()\n",
        "    # Vectorizer results are normalized, which makes KMeans behave as\n",
        "    # spherical k-means for better results. Since LSA/SVD results are\n",
        "    # not normalized, we have to redo the normalization.\n",
        "    svd = TruncatedSVD(n_components)\n",
        "    normalizer = Normalizer(copy=False)\n",
        "    lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "    X = lsa.fit_transform(X)\n",
        "\n",
        "    print(\"done in %fs\" % (time() - t0))\n",
        "\n",
        "    explained_variance = svd.explained_variance_ratio_.sum()\n",
        "    print(\"Explained variance of the SVD step: {}%\".format(\n",
        "        int(explained_variance * 100)))\n",
        "\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1Sj87MrX1-j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Then do the actual clustering and output clustering metrics\n",
        "\n",
        "Look at using [MiniBatchKmeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) insead of regular KMeans - MiniBatch means to use a smaller subset of the data over a few passes and average their results. MiniBatch has a shorter runtime than full KMeans and can actually complete on very-large datasets.\n",
        "\n",
        "Compare the clustering performance metrics between using KMeans and MiniBatchKMeans:\n",
        "* Homogeneity\n",
        "* Completeness\n",
        "* V-measure\n",
        "* Adjusted Rand-Index\n",
        "* Silhouette\n",
        "\n",
        "After you've finished examining the differing performance between KMeans and MiniBatchKMeans, try using a `k` other than `true_k`.\n",
        "\n",
        "Suggestion: Write a loop that computes and outputs the Clustering Metrics for $k \\in \\{1...30\\}$. Graph one or more of the metrics with $k$ as the `x` axis. We should see them maximize around $k=20$, the true number of clusters. Do we?\n",
        "\n",
        "Finally, after clustering, draw some exemplar data samples from each cluster and compare them in your own mind. Is the clustering process working for separating out the content of different emails?"
      ]
    },
    {
      "metadata": {
        "id": "DHOeGxRdX39c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Your implementation of Clustering Execution and Performance Metrics"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}